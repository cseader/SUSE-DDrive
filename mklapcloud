#!/bin/bash
#
# mklapcloud - Setup a virtual cloud on a laptop 
#
# Original 
# Authors: J. Daniel Schmidt <jdsn@suse.de>
#          Bernhard M. Wiedemann <bwiedemann@suse.de>
#          
# Adapted and modified:
#	   Cameron Seader <cs@suse.com>
#
# 2012, SUSE LINUX Products GmbH
#
#
# Quick introduction:
#
# This tool relies on the following.
#
# 1) the script lapcrowbarsetup.sh (in the same directory)
# 2) the repos.img (in the same directory)
# 3) the sles11sp2-64.img.gz (in the same directory)
# 4) SLES 11 SP2 DVD ISO (in the same directory)
# 5) SUSE Cloud 1.0 ISO (in the same directory)
# 6) An attached USB 3.0 hdd to create LVM's on 
# 7) A system with at least 8GB of RAM (Recommend 16GB)
# 8) openSUSE 12.1 or 12.2 installed (this does not work with SLES 11 SP2 because of the nested virtualization)
# 9) pass boot parameter kvm_intel.nested=1 for nested virtualization  
#
### Creating #2 repos.img file containing all pertinent repos for SUSE Cloud
#
# #Create the image file
# qemu-image create -f raw repos.img 40G
# 
# #Loop mount the image
# losetup /dev/loop5 repos.img
#
# #Create a partition table and a new partition in the image
# fdisk /dev/loop5
#
# #Create an ext3 filesystem on the image
# kpartx -a /dev/loop5
#  - now the partition will show up under /dev/mapper/loop5p1
# mkfs.ext3 /dev/mapper/loop5p1
#  - remove the partition from /dev/mapper
# kpartx -d /dev/loop5
# 
# #umount the loop mounted image
# losetup -d /dev/loop5
#
# # Use the documentation here https://www.suse.com/documentation/smt11/ on
# # setting up an SMT Server and mirror the following repos:
# 
# SLES11-SP1-Pool
# SLES11-SP1-Updates
# SLES11-SP2-Core
# SLES11-SP2-Updates
# SUSE-Cloud-1.0-Pool
# SUSE-Cloud-1.0-Updates
#
# # Once SMT Server is created and the above repositories are mirrored then we
# # will need to export the /srv/www/htdocs/repo/ directory over nfs or mount it
# # some way on the system which has the 40G image file we created in prior
# # steps and then follow the following steps of copying that information into
# # the image file.
#
# # I am creating some mount points to work with
# mkdir /mnt/data
# mkdir /mnt/img
# mkdir /mnt/dvd
#
# # I am mounting my repos from the smt server over nfs to /mnt/data
# mount -t nfs smt_server_hostname:/srv/www/htdocs/repo/ /mnt/data
# 
# # Mount the image file.
# losetup /dev/loop5 repos.img
# kpartx -a /dev/loop5
# mount /dev/mapper/loop5p1 /mnt/img
#
# # Now i can begin copying data from smt repos to the image file.
#cd /mnt/data
#rsync -avP \$RCE/SLES11-SP1-Pool/sle-11-x86_64/* /mnt/img/SLES11-SP1-Pool
#rsync -avP \$RCE/SLES11-SP1-Updates/sle-11-x86_64/* /mnt/img/SLES11-SP1-Updates
#rsync -avP \$RCE/SLES11-SP2-Core/sle-11-x86_64/* /mnt/img/SLES11-SP2-Core
#rsync -avP \$RCE/SLES11-SP2-Updates/sle-11-x86_64/* /mnt/img/SLES11-SP2-Updates
#rsync -avP \$RCE/SLES11-SP2-Core/sle-11-x86_64/* /mnt/img/SUSE-Cloud-1.0-Pool
#rsync -avP \$RCE/SLES11-SP2-Core/sle-11-x86_64/* /mnt/img/SUSE-Cloud-1.0-Updates
#
# # Add some required directories
# mkdir /mnt/img/Cloud
# mkdir /mnt/img/suse-11.2/install
#
# # Copy the contents of SLES 11 SP2 DVD iso to /mnt/img/suse-11.2/install
# mount -o loop SLES-11-SP2-DVD-x86_64-GM-DVD1.iso /mnt/dvd
# rsync -avP /mnt/dvd/* /mnt/img/suse-11.2/install
# umount /mnt/dvd
#
# # umount the repos.img file
# umount /mnt/img
# kpartx -d /dev/loop5
# losetup -d /dev/loop5
#
# # Cleanup
# cd
# umount /mnt/data
# rm -rf /mnt/data
# rm -rf /mnt/img
# rm -rf /mnt/dvd
# 
## The repos.img image file is ready for use
#
### Creating #3 sles11sp2-64.img.gz
# 1) Install default SLES 11 SP2 into a RAW disk image using kvm
# 2) shut down the VM once all installed
# 3) copy the VM raw image to the location where your mklapcloud script is
# 4) gzip the image with  #gzip sles11sp2-64.img
#
# Please 'export' environment variables according to your needs:
#
# CVOL=/dev/vdx  (default=/dev/vdb)
#       device where a LVM physical volume can be created
#       should be at least 80 GB
#       the volume group will be called "cloud"
#
# cloudsource=develcloud|susecloud|Beta?   (required, no default)
#       defines the source for the installation of the cloud product
#       develcloud : product from IBS Devel:Cloud
#       susecloud  : product from IBS SUSE:Cloud
#       Beta?      : uses official Beta? ISO image (? is a number)
#
# cephenable=''|1 (default='')
#                will create a separate 20GB partition for the nodes
#                for ceph testing
#
# nodenumber     (default 2)
#                sets the number of nodes to be created
#
# vcpus          (default 1)
#                sets the number of CPU cores assigned to each node (admin and compute)
#

if test `id -u` != 0 ; then
  echo "Error: This script needs to be run as root"
  echo "  Please be aware that this script will create a LVM"
  echo "  and kill all current VMs on this host."
  exit 1
fi

CVOL=${CVOL:-/dev/vdb}
if grep -q $CVOL /proc/mounts ; then
  echo "The device $CVOL seems to be used. Exiting."
  exit 92
fi
if [ ! -e $CVOL ] ; then
  echo "Error: $CVOL does not exist."
  echo "Please set the cloud volume group to an existing device: export CVOL=/dev/sdx"
  echo "Running 'partprobe' may help to let the device appear."
  exit 93
fi


if rcSuSEfirewall2 status ; then
  echo "Error: SuSEfirewall is running - it will interfere with the iptables rules done by libvirt"
  echo "Please stop the SuSEfirewall completely and run mkcloud again"
  echo "Run:  rcSuSEfirewall2 stop && insserv -r SuSEfirewall2_setup && insserv -r SuSEfirewall2_init"
  exit 91
fi

if grep "devpts.*[^x]mode=.00" /proc/mounts ; then
  echo "Error: /dev/pts is not accessible for libvirt, maybe you use autobuild on your system."
  echo "Please remount it using the following command:"
  echo " # mount -o remount,mode=620,gid=5 devpts -t devpts /dev/pts"
  exit 13
fi

adminip=192.168.124.10

SCRIPT=$(basename $0)
# must be > 2 as the backticks count as well as separate process with the same name
if [ `ps aux | grep -v -e grep -e SCREEN | grep $SCRIPT | wc -l` -gt 2 ]  ; then
  echo "Warning: mkcloud was started twice."
  echo "This is not supported ... exiting."
  echo
  echo 'Maybe you just have a "$EDITOR mkcloud" process running. Please close it.'
  exit 33
fi

if [ ! -e lapcrowbarsetup.sh ] ; then
  echo "Thank you for using $0."
  echo "For proper functionality you also need to copy the script lapcrowbarsetup.sh here."
  exit 87
fi

nodenumber=${nodenumber:-2}
allnodeids=`seq 1 $nodenumber`
vcpus=${vcpus:-1}

if [ $nodenumber -gt 4 ] ; then
  echo "Error: maximal 4 nodes supported. You requested $nodenumber."
  exit 7
fi

cpuflags="<cpu mode='custom' match='exact'>
    <model>qemu64</model>
    <feature policy='require' name='vmx'/>
    <feature policy='disable' name='svm'/>
  </cpu>"

### cleanup function ###

function cleanup()
{
  # cleanup leftover from last run
  allnodenames=`for i in $allnodeids ; do echo -n "node\$i " ; done`
  for n in admin $allnodenames ; do virsh destroy cloud-$n ; virsh undefine cloud-$n ; done ; virsh net-destroy cloud-admin ; rclibvirtd stop ; killall -9 libvirtd ; ifdown virbr1 ; brctl delbr virbr1 ; killall dnsmasq qemu-kvm ; iptables -F FORWARD ; iptables -F POSTROUTING -t nat
  # zero node volumes to prevent accidental booting
  for node in $allnodeids ; do
    dd if=/dev/zero of=/dev/cloud/node$node count=1 bs=512
  done
  # zero admin volumes
  for n in admin $allnodenames ; do
    dd if=/dev/zero of=/dev/cloud/cloud-$node count=1 bs=512
  done
#  umount /mnt
#  losetup -d /dev/loop0
  rm -f /var/run/libvirt/qemu/cloud*.xml /var/lib/libvirt/network/cloud*.xml
  return 0
}

function prepare()
{
# The line below can be commented out in order to skip this step when running it multiple times.   
#zypper --non-interactive in --no-recommends libvirt kvm lvm2 wget bridge-utils vlan dnsmasq netcat-openbsd ebtables

  grep -q NumberOfPasswordPrompts ~/.ssh/config 2>/dev/null || cat > ~/.ssh/config <<EOSSH
Host crowbar $adminip
NumberOfPasswordPrompts 0
UserKnownHostsFile /dev/null
StrictHostKeyChecking no
EOSSH

  pvcreate $CVOL
  vgcreate cloud $CVOL
  vgchange -ay cloud 

  lvcreate -n admin -L 20G cloud
  lvcreate -n admin-repos -L 40G cloud
  for i in $allnodeids ; do
    lvcreate -n node$i -L 20G cloud
  done

  if [ ! -z $cephenable ] ; then
    for i in $allnodeids ; do
      lvcreate -n node$i-ceph -L 10G cloud
    done
  fi

  if [ -e repos.img ] ; then
    echo "Starting block copy of repos.img to /dev/cloud/admin-repos"
    time dd if=repos.img of=/dev/cloud/admin-repos bs=64k
    else 
    echo "you need to supply the repos.img in the same location as this script."
    exit 3 
  fi

  if [ -e sles11sp2-64.img.gz ] ; then
    echo "Starting block copy of sles11sp2-64.img.gz to /dev/cloud/admin"
    time gzip -cd sles11sp2-64.img.gz | dd of=/dev/cloud/admin
    else 
    echo "you need to supply the sles11sp2-64.img.gz in the same location as this script."
    exit 3 
  fi

 # make a bigger partition 2 on /dev/cloud/admin
  echo -e "d\n2\nn\np\n2\n\n\na\n2\nw" | fdisk /dev/cloud/admin
 #losetup -o $(expr 1028096 \* 512) /dev/loop0 /dev/cloud/admin
  kpartx -a /dev/cloud/admin
  fsck -f /dev/mapper/cloud-admin2
  resize2fs /dev/mapper/cloud-admin2
  sync
  kpartx -d /dev/cloud/admin

# make a bigger partition 1 on /dev/cloud/admin-repos (fdisk would not partition correctly)
# echo -e "d\nn\np\n1\n\n\nw" | fdisk /dev/cloud/admin-repos
  kpartx -a /dev/cloud/admin-repos
  fsck -n /dev/mapper/cloud-admin--repos1
  tune2fs -O ^has_journal /dev/mapper/cloud-admin--repos1
  fsck -f /dev/mapper/cloud-admin--repos1
  parted /dev/mapper/cloud-admin--repos -s print
  parted /dev/mapper/cloud-admin--repos -s rm 1
  parted /dev/mapper/cloud-admin--repos -s print
  parted /dev/mapper/cloud-admin--repos -s -- mkpart primary 1049kB -1
  parted /dev/mapper/cloud-admin--repos -s print
  kpartx -a /dev/cloud/admin-repos
  fsck /dev/mapper/cloud-admin--repos1
  resize2fs -p /dev/mapper/cloud-admin--repos1
  tune2fs -j /dev/mapper/cloud-admin--repos1
  fsck -f /dev/mapper/cloud-admin--repos1
  sync
  kpartx -d /dev/cloud/admin-repos

# setup /etc/hosts for crowbar(admin node) and all nodes
  grep -q crowbar /etc/hosts || echo "$adminip  admin.cloud.selab.net admin" >> /etc/hosts
  for i in $allnodeids ; do
    grep -q node$i /etc/hosts || echo "192.168.124.8$i  node$i.cloud.selab.net node$i" >> /etc/hosts
  done
}

function sshrun()
{
  ssh root@$adminip "export nodenumber=$nodenumber ; $@"
  return $?
}

function setupadmin()
{
  echo "Creating key for controlling our VMs..."
  [ -e ~/.ssh/id_dsa ] || ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ""
  echo "Injecting public key into image..."
  pubkey=`cut -d" " -f2 ~/.ssh/id_dsa.pub`
  kpartx -a /dev/cloud/admin
  mount /dev/mapper/cloud-admin2 /mnt
  mkdir -p /mnt/root/.ssh
  grep -q $pubkey /mnt/root/.ssh/authorized_keys2 2>/dev/null || cat ~/.ssh/id_dsa.pub >> /mnt/root/.ssh/authorized_keys2
  umount /mnt
  kpartx -d /dev/cloud/admin
  sync

  cat > /tmp/cloud-admin.xml <<EOLIBVIRT
<domain type='kvm'>
  <name>cloud-admin</name>
  <uuid>07e939e7-57eb-4da0-dc94-c7c8faf03693</uuid>
  <memory>2097152</memory>
  <currentMemory>2097152</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.12'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-kvm</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/cloud/admin'/>
      <target dev='vda' bus='virtio'/>
    </disk>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/cloud/admin-repos'/>
      <target dev='vdb' bus='virtio'/>
    </disk>
    <interface type='network'>
      <mac address='52:54:00:0d:36:5b'/>
      <source network='cloud-admin'/>
      <model type='virtio'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>
EOLIBVIRT

  # dont specify range
  # this allows to use the same network for cloud-nodes that get DHCP from crowbar
  # doc: http://libvirt.org/formatnetwork.html
  cat > /tmp/cloud-admin.net.xml <<EOLIBVIRTNET
  <network>
    <name>cloud-admin</name>
    <uuid>76b08f53-4fe0-3bb6-8220-d4cfc7b23423</uuid>
    <bridge name='virbr1' stp='on' delay='0' />
    <mac address='3C:97:0E:2D:D5:E7'/>
    <ip address='192.168.124.1' netmask='255.255.255.0'>
      <dhcp>
        <host mac="52:54:00:0d:36:5b" name="admin.cloud.selab.net" ip="$adminip"/>
      </dhcp>
    </ip>
    <forward mode='nat'>
    </forward>
  </network>
EOLIBVIRTNET

#  modprobe kvm-amd
#  insserv libvirtd
  rclibvirtd start
  echo -n "Waiting for libvirt startup: "
  n=100 ; while test $n -gt 0 && ! [ -S /var/run/libvirt/libvirt-sock ] ; do
    echo -n .
    sleep 3
  done
  echo
  if [ $n = 0 ] ; then
    echo "Error: libvirtd socket was not created within reasonable time."
    exit 71
  else
    echo "Libvirtd socket created. Continuing."
  fi

  [ -e /etc/libvirt/qemu/networks/cloud-admin.xml ] || virsh net-define /tmp/cloud-admin.net.xml
  if ! virsh net-start cloud-admin ; then
    echo "=====================================================>>"
    echo "Error:Could not start Network for: cloud:admin"
    exit 75
  fi 

  [ -e /etc/libvirt/qemu/cloud-admin.xml ] || virsh define /tmp/cloud-admin.xml
  if ! virsh start cloud-admin ; then
    echo "=====================================================>>"
    echo "Error: Could not start VM for: cloud-admin"
    exit 76
  fi

  #sleep 60 # time to boot up
  #adminip=`grep 52:54:00:77:77:70 /var/lib/libvirt/dnsmasq/cloud-admin.leases | cut -d" " -f3`
  echo "waiting for crowbar admin VM to become reachable"
  n=300 ; while test $n -gt 0 && ! ping -q -c 1 -w 1 $adminip >/dev/null ; do
    n=$(expr $n - 1)
    echo -n "."
  done
  #grep "iptables -t nat -F PREROUTING" /etc/init.d/boot.local || echo 'iptables -t nat -F PREROUTING ; for i in 22 80 3000 4000 4040 ; do iptables -I FORWARD -p tcp --dport $i -j ACCEPT ; for host in 10 81 82 ; do iptables -t nat -I PREROUTING -p tcp --dport $(expr $i + $host % 10 + 1100) -j DNAT --to-destination 192.168.124.$host:$i ; done ; done ; echo 0 > /proc/sys/net/ipv4/conf/all/rp_filter' >> /etc/init.d/boot.local
  #/etc/init.d/boot.local

  n=150 ; while test $n -gt 0 && ! nc -z $adminip 22 ; do
    sleep 1
    n=$(expr $n - 1)
    echo -n "."
  done

  if [ $n = 0 ] ; then
    echo "admin VM not reachable - something must have gone wrong... exiting"
    exit 57
  fi

  echo "waiting some more for sshd+named to start"
  sleep 25
  echo "you can now proceed with installing crowbar"
}

function instcrowbar()
{
  echo "connecting to crowbar admin server at $adminip"
  # scp -p SUSE-CLOUD-*.iso root@$adminip:/tmp/
  scp -p SUSE-CLOUD-1-x86_64-GM-DVD1.iso root@$adminip:/tmp/
  # copy over supplemental script to admin server
  scp lapcrowbarsetup.sh root@$adminip:
  sshrun "echo `hostname` > cloud ; preparecrowbar=1 bash -x lapcrowbarsetup.sh virtual"
  # wait for admin node to become reachable after applying patches and a reboot
   
  echo "waiting for crowbar admin VM to become reachable"
  n=300 ; while test $n -gt 0 && ! ping -q -c 1 -w 1 $adminip >/dev/null ; do
    n=$(expr $n - 1)
    echo -n "."
  done
  # Checking for port 22 (SSH)
  sleep 60
  n=300 ; while test $n -gt 0 && ! nc -z $adminip 22 ; do
    sleep 1
    n=$(expr $n - 1)
    echo -n "."
  done

  if [ $n = 0 ] ; then
    echo "admin VM not reachable - something must have gone wrong... exiting"
    exit 58
  fi
  
  echo "waiting some more for sshd to start"
  sleep 25
  echo "proceeding now to install crowbar"

  sshrun "echo `hostname` > cloud ; installcrowbar=1 bash -x lapcrowbarsetup.sh virtual"

  return $?
}

function mkvlan()
{
  DEFVLAN=$1 ; shift
  IP=$1 ; shift
  cat > /etc/sysconfig/network/ifcfg-virbr1.$DEFVLAN <<EONET
# VLAN Interface for the xxx network
MANAGED='false'
USERCONTROL='no'
STARTMODE='auto'
BOOTPROTO='static'
ETHERDEVICE='virbr1'
IPADDR='$IP/24'
VLAN_ID=$DEFVLAN
EONET

  ifup virbr1.$DEFVLAN
}

function setupcompute()
{
  # public = 300
  mkvlan 300 192.168.122.1
  # nova-fixed = 500
  mkvlan 500 192.168.123.254

  for i in $allnodeids ; do
    cephvolume="<disk type='block' device='disk'>
      <serial>cloud-node$i-ceph</serial>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/cloud/node$i-ceph'/>
      <target dev='vdb' bus='virtio'/>
    </disk>"
    if [ -z $cephenable ] ; then
      cephvolume=''
    fi

    cat > /tmp/cloud-node$i.xml <<EOLIBVIRT
<domain type='kvm'>
  <name>cloud-node$i</name>
  <uuid>07e939e7-57e$i-4da0-dc94-c7c8faf03693</uuid>
  <memory>4194304</memory>
  <currentMemory>2097152</currentMemory>
  <vcpu>$vcpus</vcpu>
  <os>
    <type arch='x86_64' machine='pc-0.12'>hvm</type>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  $cpuflags
  <clock offset='utc'/>
  <on_poweroff>preserve</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/qemu-kvm</emulator>
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw'/>
      <source dev='/dev/cloud/node$i'/>
      <target dev='vda' bus='virtio'/>
      <boot order='2'/>
    </disk>
    $cephvolume
    <interface type='network'>
      <mac address='52:54:00:0d:36:5$i'/>
      <source network='cloud-admin'/>
      <model type='virtio'/>
      <boot order='1'/>
    </interface>
    <serial type='pty'>
      <target port='0'/>
    </serial>
    <console type='pty'>
      <target type='serial' port='0'/>
    </console>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
    </video>
    <memballoon model='virtio'>
    </memballoon>
  </devices>
</domain>
EOLIBVIRT

    virsh destroy cloud-node$i 2>/dev/null
    if ! virsh create /tmp/cloud-node$i.xml ; then
      echo "====>>"
      echo "Error: Could not create VM for: node$i"
      exit 74
    fi
  done
  sleep 20
  for i in $allnodeids ; do # work around bridge forward delay or something causing pxelinux transfer fail on one node
    [ $i = 1 ] && continue
    virsh send-key cloud-node$i KEY_RIGHTCTRL KEY_RIGHTALT KEY_DELETE
    sleep 1
  done

  sleep 20
  scp lapcrowbarsetup.sh root@$adminip:
  sshrun "allocate=1 bash -x lapcrowbarsetup.sh virtual"
  ret=$?
  [ $ret != 0 ] && return $ret

  echo "Waiting for the installation of the nodes ..."
  sshrun '
  for i in `seq $nodenumber` ; do
    echo -n "Waiting for node $i: "
    while ! ssh -o NumberOfPasswordPrompts=0 -o "StrictHostKeyChecking no" 192.168.124.8$i rpm -q yast2-core 2>/dev/null 1>&2; do
      sleep 10
      echo -n "."
    done
    echo "node $i ready"
  done
  '
  return $?

  return 0
}


function proposal()
{
  scp lapcrowbarsetup.sh root@$adminip:
  sshrun "proposal=1 bash -x lapcrowbarsetup.sh virtual"
  return $?
}

function testsetup()
{
  scp lapcrowbarsetup.sh root@$adminip:
  sshrun "testsetup=1 bash -x lapcrowbarsetup.sh virtual"
  return $?
}


function usage()
{
  echo "Usage:"
  echo "$0 <command> [<command>,...]"
  echo
  echo "  'command' is one of:"
  echo "   all instonly plain cleanup prepare setupadmin instcrowbar setupcompute proposal testsetup help"
  echo
  echo "   all      -> expands to: plain testsetup"
  echo "   plain    -> expands to: cleanup prepare setupadmin instcrowbar setupcompute proposal"
  echo "   instonly -> expands to: cleanup prepare setupadmin instcrowbar setupcompute"
  echo
  echo "   cleanup:     kill all running VMs, zero out boot sectors of all lvm volumes"
  echo "   prepare:     create lvm volumes, setup libvirt networks"
  echo "   setupadmin:  create the admin node and install the cloud product"
  echo "   instcrowbar: install crowbar and chef on the admin node"
  echo "   setupcompute:create the compute nodes and let crowbar install them"
  echo "   testsetup:   start a VM in the cloud"
  echo "   help:        this usage"
  echo
  echo " Environment variables (need to be exported):"
  echo
  echo " Mandatory"
  echo "   CVOL=/dev/vdx (default /dev/vdb)"
  echo "       :  LVM will be created on this device (at least 80GB)"
  echo
  echo " Optional"
  echo "   cephenable='' | 1  (default='')"
  echo "       : create a separate 10GB partition per node for ceph"
  echo "         note: proposal step does NOT contain a ceph proposal, do it manually"
  echo "   nodenumber=2    (default 2)"
  echo "       : set the number of nodes to be created (excl. admin node)"
  echo "   vcpus=1         (default 1)"
  echo "       : set the number of CPU cores per node (admin and compute)"
  echo
  exit 1
}

## MAIN ##

allcmds="all instonly plain cleanup prepare setupadmin instcrowbar setupcompute proposal testsetup help"
wantedcmds=$@
runcmds=''

# parse the commands and expand the aliases
for cmd in $wantedcmds ; do
  if [ "$cmd" = "help" -o "$cmd" = "--help" -o "$cmd" = "usage" ] ; then
    usage
  fi

  ok=0
  for onecmd in $allcmds ; do
    if [ $onecmd = $cmd ] ; then
      ok=1
      case $cmd in
        all)
          runcmds="$runcmds cleanup prepare setupadmin instcrowbar setupcompute proposal testsetup"
        ;;
        plain)
          runcmds="$runcmds cleanup prepare setupadmin instcrowbar setupcompute proposal"
        ;;
        instonly)
          runcmds="$runcmds cleanup prepare setupadmin instcrowbar setupcompute"
        ;;
        *)
          runcmds="$runcmds $cmd"
        ;;
      esac
    fi
  done

  if [ $ok = 0 ] ; then
    echo "Error: Command $cmd unknown."
    usage
  fi
done

echo "You choose to run these mkcloud steps:"
echo "  $runcmds"
echo
sleep 2

for cmd in `echo $runcmds` ; do
  echo
  echo "============> MKCLOUD STEP START <============"
  echo
  echo "Now running step: $cmd"
  echo
  sleep 2
  $cmd
  ret=$?
  if [ $ret != 0 ] ; then
    echo
    echo '$h1!!'
    echo "Error detected. Stopping mkcloud."
    echo "The step '$cmd' returned with exit code $ret"
    echo "Please refer to the $cmd function in this script when debugging the issue."
    echo
    exit $ret
  fi
  echo
  echo "^^^^^^^^^^^^= MKCLOUD STEP DONE  =^^^^^^^^^^^^"
  echo
done
